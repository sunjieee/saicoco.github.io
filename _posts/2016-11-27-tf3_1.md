---
title: " tensorflow使用笔记(3)--Reading data实例(2)"
layout: post
date: 2016-11-27
image: /assets/images/markdown.jpg
headerImage: false
tag: tensorflow
category: tensorflow
blog: true
start: true
author: karl
description: 读取数据，从文件，从队列
---  

接上篇， 这里实现了一个手写数字版本，和官方的差不多，但都是亲手敲的。  

### code
---
```python
# -*- coding: utf-8 -*-
# @author: jiajia geng
# @Email: jiajia.geng@yahoo.com

import tensorflow as tf
from tensorflow.contrib.learn.python.learn.datasets import mnist
import logging
import time

flags = tf.app.flags
FLAGS = flags.FLAGS
flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')
flags.DEFINE_integer('num_epochs', 2, 'Number of epochs to run trainer.')
flags.DEFINE_integer('hidden1', 128, 'Number of units in hidden layer 1.')
flags.DEFINE_integer('hidden2', 32, 'Number of units in hidden layer 2.')
logging.basicConfig(level=logging.INFO)

def _int64_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def _bytes_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def convert_data(data, name):
    images = data.images
    labels = data.labels
    num_examples = data.num_examples

    assert images.shape[0]==num_examples
    num_examples, feature_length = images.shape

    filename = './data/' + name + '.tfrecords'
    logging.info('writing...')

    writer = tf.python_io.TFRecordWriter(filename)
    for index in xrange(num_examples):
        image_raw = images[index].tostring()
        logging.info('images-{} write in'.format(index))
        example = tf.train.Example(
            features = tf.train.Features(
                feature={
                    'length': _int64_feature(feature_length),
                    'label': _int64_feature(int(labels[index])),
                    'image_raw': _bytes_feature(image_raw)
                }
            )
        )

        writer.write(example.SerializeToString())
    writer.close()


def read_data(filename_queue):
    reader = tf.TFRecordReader()
    _, example = reader.read(filename_queue)
    # parse from records file and get a example of records.

    features = tf.parse_single_example(
        example,
        features={
            'image_raw': tf.FixedLenFeature([], tf.string), # tf.FixedLenFeature返回字典
            'label': tf.FixedLenFeature([], tf.int64)
        }
    )
    images = tf.decode_raw(features['image_raw'], tf.uint8) # 将byte转换为各种类型
    images = tf.cast(images, tf.float32) * (1./255.)
    labels = features['label']
    return images, labels

def mlp(images, labels):

    w_1 = tf.Variable(tf.truncated_normal(shape=(3136, 500), stddev=0.01), name='w_1')
    w_2 = tf.Variable(tf.truncated_normal(shape=(500, 10), stddev=0.01), name='w_2')

    b_1 = tf.zeros(shape=(500,), name='b_1')
    b_2 = tf.zeros(shape=(10,), name='b_2')

    h1 = tf.nn.relu(tf.matmul(images, w_1) + b_1)
    h2 = tf.matmul(h1, w_2) + b_2

    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=h2, labels=labels)
    loss = tf.reduce_mean(losses)
    acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(h2, 1), labels), tf.float32))

    trainer = tf.train.AdamOptimizer(0.01).minimize(loss)
    return trainer, loss, acc

    pass
if __name__ == '__main__':
    # datasets = mnist.read_data_sets(train_dir='./data', reshape=True)
    # print datasets.train.labels.shape
    # convert_data(datasets.train, 'train')

    filename_queue = tf.train.string_input_producer(['./data/train.tfrecords'], num_epochs=10)
    images, labels = read_data(filename_queue)

    train_x, train_y = tf.train.shuffle_batch(
        tensors=[images, labels],
        shapes=[(3136), ()], # 必须指定
        batch_size=128,
        num_threads=4,
        capacity=1000 + 3*128,
        min_after_dequeue=1000,
    )

    MLP, loss, acc = mlp(train_x, train_y)

    init = tf.group(
        tf.initialize_all_variables(),
        tf.initialize_local_variables()
    )
    sess = tf.Session()
    sess.run(init)
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    try:
        step = 0

        while not coord.should_stop():
            start_time = time.time()
            _, loss_val, acc_val = sess.run([MLP, loss, acc])

            duration = time.time() - start_time

            if step%100 == 0:
                logging.info('step:{}, loss:{}, acc: {}'.format(step, loss_val, acc_val))
            step += 1
    except tf.errors.OutOfRangeError:
        logging.info('Done training for {} epoches, {} steps'.format(10, step))
    finally:
        coord.request_stop()

    coord.join(threads=threads)
    sess.close()

```   

---  

###  代码说明  

#### Feature  

`Feature`来自文件[feature.proto](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/core/example/feature.proto), 这里截取局部：  

```
message Feature {
  // Each feature can be exactly one kind.
  oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
  }
};

message Features {
  // Map from feature name to feature.
  map<string, Feature> feature = 1;
};
```  

即代码中`tf.train.Feature`表示的就是此处的Feature, 可以看到Feature有三种类型，分别为BytesList, FloatList, Int64List，而且只能从中选取一个。对于`Features`,类型则是字典类型，字典中的每一个元素均是Feature类型。那么问题来了， XXXList又是啥：  

```
message BytesList {
  repeated bytes value = 1;
}
message FloatList {
  repeated float value = 1 [packed = true];
}
message Int64List {
  repeated int64 value = 1 [packed = true];
}
```  

由上面可以看到，List对应其实就是一些值。接下来我们就可以说`tf.train.Example`:  

```python
example = tf.train.Example(
    features = tf.train.Features(
        feature={
            'length': _int64_feature(feature_length),
            'label': _int64_feature(int(labels[index])),
            'image_raw': _bytes_feature(image_raw)
        }
    )
)
```

上述代码什么意思，意思就是声明一个数据实例，用于写入records文件中，而example中需要参数features,而features有proto中的Features构成，而`tf.train.Features`中feature为字典，给出键值对即可，而feature类型只有三种，因此存储数据是需要将数据转换成这三种类型才可以进行存储。  

### 队列shuffle  

```python
train_x, train_y = tf.train.shuffle_batch(
    tensors=[images, labels],
    shapes=[(3136), ()], # 必须指定
    batch_size=128,
    num_threads=4,
    capacity=1000 + 3*128,
    min_after_dequeue=1000,
)
```
`shuffle_batch`的作用在于生成一系列的tensor,用作后续的run数据的供给．

这里是我代码运行是提示的错误，因为mnist 大小为(None, 784)，因此这里我指定了shapes，当指定784时会报错，错误为与3136维度不匹配，意思是出队列数据维度为3136，我猜想是对线程的原因，一口气出四个，于是将网络入口参数改为了3136，可以运行，准确率也不错，这块还需要自己想想原因，但愿是多线程。  

## 总结　　
总而言之，TFRecords的读写可以总结为以下两点:  
* 写入：获取数据->构造tf.train.Example并填入Feature->序列化SerializeToString()->写入TFRecords;  
* 读入：获取TFRecords文件的队列(tf.train.string_input_producer)->读取Exmaple->解析序列parse_single_example->获取数据->构造输入Tensor(tf.train.shuffle_batch)->sess.run()

好啦，reading_data就到这里了。

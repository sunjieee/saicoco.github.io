---
layout: post
title: 利用RNN实现手写数字识别
date: 2016-07-19
tag: python
category: RNN
comments: true
blog: true
---

要点：

* `SimpleRNN`的使用：  
`keras.layers.recurrent.SimpleRNN(output_dim, init='glorot_uniform', inner_init='orthogonal', activation='tanh', W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)
`
`input_dim=28`,`time_step=28`即一张$$28 \times 28$$的图分为28个时间步

* `keras.layers.core.TimeDistributedDense(output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)
`  
该层的输入应为`input_shape=(n_samples, time_steps, input_dim)`, 使用时前一层`rnn_layer`需要设置`return_sequences=True`,这样返回每次rnn时间步的输出，作为
`TimeDistributed`的输入，经过实验，手写数字适合所有时间步结束时再做loss,进而进行梯度下降。

如下代码时使用`TimeDistributed`:  

``` python
import numpy
import cPickle
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Activation, Convolution2D, ZeroPadding2D, TimeDistributed
from keras.optimizers import RMSprop
from keras.preprocessing import sequence
def to_categorical(y, nb_classes=None):
    '''Convert class vector (integers from 0 to nb_classes)
    to binary class matrix, for use with categorical_crossentropy.
    '''
    if not nb_classes:
        nb_classes = numpy.max(y)+1
    Y = numpy.zeros((len(y), nb_classes))
    for i in range(len(y)):
        Y[i, y[i]] = 1.
    return Y

with open('mnist.pkl', 'r') as f:
    _train, _val, _test = cPickle.load(f)
train_x, train_y = _train
val_x, val_y = _val
train_y = to_categorical(train_y, nb_classes=10)
val_y = to_categorical(val_y, nb_classes=10)
train_x = train_x.reshape((50000, 28, 28))/255.
val_x = val_x.reshape((10000, 28, 28))/255.
train_y = numpy.tile(train_y, 28).reshape(train_y.shape[0], 28, 10)
val_y = numpy.tile(val_y, 28).reshape(val_y.shape[0], 28, 10)
shape = train_x.shape[1:]

model = Sequential()
model.add(SimpleRNN(input_shape=shape, output_dim=50, return_sequences=True))
model.add(TimeDistributed(Dense(output_dim=10)))
model.add(Activation('softmax'))
sgd = RMSprop()
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_x, train_y, nb_epoch=200, batch_size=100, verbose=1, validation_data=(val_x, val_y))
```  
对于跑完时间步才计算损失的，可以使用简单`Dense_layer`

而对于CNN与RNN的对接，可以通过`Lambda`来实现，对得到的feature map 进行`reshape`，使得每一个map的维度均是一个`input_dim`,
下面是使用CNN与RNN对接的代码：

```python
import numpy
import cPickle
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense, Activation, Convolution2D, ZeroPadding2D, TimeDistributed, Lambda
from keras.optimizers import RMSprop
def to_categorical(y, nb_classes=None):
    if not nb_classes:
        nb_classes = numpy.max(y)+1
    Y = numpy.zeros((len(y), nb_classes))
    for i in range(len(y)):
        Y[i, y[i]] = 1.
    return Y

def cnn2rnn(x):
    shape = x.shape
    time_steps = shape[1] * shape[2]
    input_dim = shape[3]
    return x.reshape((shape[0], time_steps, input_dim))

def output_shape(input_shape):
    time_steps = input_shape[1] * input_shape[2]
    input_dim = input_shape[3]
    return (None, time_steps, input_dim)

with open('mnist.pkl', 'r') as f:
    _train, _val, _test = cPickle.load(f)
train_x, train_y = _train
val_x, val_y = _val
train_y = to_categorical(train_y, nb_classes=10)
val_y = to_categorical(val_y, nb_classes=10)
train_x = train_x.reshape((50000, 1, 28, 28))/255.
val_x = val_x.reshape((10000, 1, 28, 28))/255.
map2step = 8*28
train_y = numpy.tile(train_y, map2step).reshape(train_y.shape[0], map2step, 10)
val_y = numpy.tile(val_y, map2step).reshape(val_y.shape[0], map2step, 10)
shape = train_x.shape[1:]

# model
model = Sequential()
model.add(ZeroPadding2D(padding=(1, 1), input_shape=(1, 28, 28)))
model.add(Convolution2D(8, 3, 3))
model.add(ZeroPadding2D(padding=(1, 1)))
model.add(Convolution2D(8, 3, 3))
model.add(Lambda(cnn2rnn, output_shape=output_shape))
model.add(SimpleRNN(input_shape=shape, output_dim=50, return_sequences=True))
# model.add(Dense(input_dim=50, output_dim=10))
model.add(TimeDistributed(Dense(output_dim=10)))
model.add(Activation('softmax'))
rms = RMSprop(lr=0.003)
model.compile(optimizer=rms, loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(train_x, train_y, nb_epoch=200, batch_size=100, verbose=1, validation_data=(val_x, val_y))
```
关键在于RNN中one2one, one2many, many2one, many2many这几个类型的对接处的不同。详情参看[issue](https://github.com/fchollet/keras/issues/2198)

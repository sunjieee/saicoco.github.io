---
title: "Mxnet使用中的那些小事--持续更新中"
layout: post
date: 2017-02-23
tag: mxnet
blog: true
star: true
author: karl
category: mxnet
description: 自定义Op
---  　

最近在用mxnet重写代码，因为之前的代码因为集群问题没法跑了，想方设法找到一个可以在集群跑的框架，只有mxnet。于是
就开始了填坑之旅。想着开篇文章记录一下这个过程，用于以后查阅，反省。　　

## 关于symbol.infer_shape  

infer_shape可以用于测试自己写的symbol，而这里有些参数说明。文档中有说明但是不够具体，说白了就一句话，里面的
参数就是构建symbol时的入口variable的name,当然大部分情况name是默认data,label;有些时候我们自定义DataIter后，这里
name就是我们自定义的name.来个例子说一下吧：　　

```python
audio_data = mx.sym.Variable('audio_data')
face_data = mx.sym.Variable('face_data')
softmax_label = mx.sym.Variable('softmax_label')
# LSTM1, LSTM2
rnn1 = mx.rnn.LSTMCell(num_hidden=256, prefix='lstm1_')
rnn1_outputs, rnn1_states = rnn1.unroll(length=49, inputs=audio_data, merge_outputs=False)
rnn2 = mx.rnn.LSTMCell(num_hidden=256, prefix='lstm2_')
rnn2_outputs, rnn2_states = rnn2.unroll(length=49, inputs=rnn1_outputs, merge_outputs=False)
rnn1_last_out = mx.sym.Reshape(data=rnn1_outputs[-1], shape=(-1, 256), name='rnn1_last_out_reshape')
rnn2_last_out = mx.sym.Reshape(data=rnn2_outputs[-1], shape=(-1, 256), name='rnn2_last_out_reshape')
rnn_outputs = mx.sym.Concat(rnn1_last_out, rnn2_last_out, num_args=2, dim=1, name='rnn_outputs')

audio_data_shape = (128, 49, 75)
face_data_shape = (128, 512, 14, 14)
label_data_shape = (128, 6)
rnn_outputs_shape = rnn_outputs.infer_shape(audio_data=audio_data_shape)
```

这里`rnn_outputs_shape`如下所示：　　
```python
INFO:root:rnn_outputs_shape:([(128L, 49L, 75L), (1024L, 75L), (1024L,), (1024L, 256L), (1024L,), (1024L, 256L), (1024L,), (1024L, 256L), (1024L,)], [(128L, 512L)], [])
```

分别指代input_shape, output_shape, aux_shape,为了获得对应的shape,只需要取对应的index即可。  

## 关于自定义data_names, label_names  

在自定义DataIter后，对于数据的入口可能由默认的data, label变成了自定义的形式，因此需要在`mod.fit`之前声明，通常格式
为：　　
```python
Module(label_names=('new_label',), data_names=('new_data_name',)...)
```

这里需要注意的是，元祖若只有一个元素，需要有逗号：('new_label',),否则会报错：　　

```
ValueError: Unknown initialization pattern for softmax_label. Default initialization is now limited to "weight", "bias", "gamma" (1.0), and "beta" (0.0).
Please use mx.sym.Variable(init=mx.init.*) to set initialization pattern
```

## 关于自定义loss通过mx.sym.MakeLoss  

先列出检索到的相关issue:  

1.[issue4737](https://github.com/dmlc/mxnet/issues/4737)  
2.[stackoverflow](http://stackoverflow.com/questions/42304820/how-to-weight-observations-in-mxnet/42323339#42323339)

```python
# -*- coding=utf-8 -*-

import mxnet as mx
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

x = mx.sym.Variable('data')
y = mx.sym.FullyConnected(data=x, num_hidden=1)
label = mx.sym.Variable('label')
loss = mx.sym.MakeLoss(mx.sym.square(y - label))
pred_loss = mx.sym.Group([mx.sym.BlockGrad(y), loss])
ex = pred_loss.simple_bind(mx.cpu(), data=(32, 2))

# test
test_data = mx.nd.array(np.random.random(size=(32, 2)))
test_label = mx.nd.array(np.random.random(size=(32, 1)))

ex.forward(is_train=True, data=test_data, label=test_label)
ex.backward()

print ex.arg_dict
fc_weights = ex.arg_dict['fullyconnected0_weight'].asnumpy()
fc_weights_grad = ex.grad_arrays[1].asnumpy()
fc_bias = ex.arg_dict['fullyconnected0_bias'].asnumpy()
fc_bias_grad = ex.grad_arrays[2].asnumpy()

logging.info('fc_weight:{}, fc_weights_grad:{}'.format(fc_weights, fc_weights_grad))
logging.info('fc_bias:{}, fc_bias_grad:{}'.format(fc_bias, fc_bias_grad))
```

以上是自己写的小栗子。需要几点来说明：

mx.sym.MakeLoss仅仅相当于自定义Loss,并没有涉及到prediction,因此在想获取prediction时，需要通过Group将prediction=mx.BlockGrad(y)和
loss合并起来。

* [mx.sym.BlockGrad](http://mxnet.io/api/python/symbol.html#mxnet.symbol.BlockGrad):用于获取symbol的输出，并且回传0梯度。　　
* [mx.sym.Group](http://mxnet.io/api/python/symbol.html#mxnet.symbol.Group):返回symbol列表

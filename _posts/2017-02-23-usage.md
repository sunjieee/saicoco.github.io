---
title: "Mxnet使用中的那些小事--持续更新中"
layout: post
date: 2017-02-23
tag: mxnet
blog: true
star: true
author: karl
category: mxnet
description: 自定义Op
---


最近在用mxnet重写代码，因为之前的代码因为集群问题没法跑了，想方设法找到一个可以在集群跑的框架，只有mxnet。于是
就开始了填坑之旅。想着开篇文章记录一下这个过程，用于以后查阅，反省。　　

## 关于symbol.infer_shape  

infer_shape可以用于测试自己写的symbol，而这里有些参数说明。文档中有说明但是不够具体，说白了就一句话，里面的
参数就是构建symbol时的入口variable的name,当然大部分情况name是默认data,label;有些时候我们自定义DataIter后，这里
name就是我们自定义的name.来个例子说一下吧：　　

```python
audio_data = mx.sym.Variable('audio_data')
face_data = mx.sym.Variable('face_data')
softmax_label = mx.sym.Variable('softmax_label')
# LSTM1, LSTM2
rnn1 = mx.rnn.LSTMCell(num_hidden=256, prefix='lstm1_')
rnn1_outputs, rnn1_states = rnn1.unroll(length=49, inputs=audio_data, merge_outputs=False)
rnn2 = mx.rnn.LSTMCell(num_hidden=256, prefix='lstm2_')
rnn2_outputs, rnn2_states = rnn2.unroll(length=49, inputs=rnn1_outputs, merge_outputs=False)
rnn1_last_out = mx.sym.Reshape(data=rnn1_outputs[-1], shape=(-1, 256), name='rnn1_last_out_reshape')
rnn2_last_out = mx.sym.Reshape(data=rnn2_outputs[-1], shape=(-1, 256), name='rnn2_last_out_reshape')
rnn_outputs = mx.sym.Concat(rnn1_last_out, rnn2_last_out, num_args=2, dim=1, name='rnn_outputs')

audio_data_shape = (128, 49, 75)
face_data_shape = (128, 512, 14, 14)
label_data_shape = (128, 6)
rnn_outputs_shape = rnn_outputs.infer_shape(audio_data=audio_data_shape)
```

这里`rnn_outputs_shape`如下所示：　　
```python
INFO:root:rnn_outputs_shape:([(128L, 49L, 75L), (1024L, 75L), (1024L,), (1024L, 256L), (1024L,), (1024L, 256L), (1024L,), (1024L, 256L), (1024L,)], [(128L, 512L)], [])
```

分别指代input_shape, output_shape, aux_shape,为了获得对应的shape,只需要取对应的index即可。  

## 关于自定义data_names, label_names  

在自定义DataIter后，对于数据的入口可能由默认的data, label变成了自定义的形式，因此需要在`mod.fit`之前声明，通常格式
为：　　
```python
Module(label_names=('new_label',), data_names=('new_data_name',)...)
```

这里需要注意的是，元祖若只有一个元素，需要有逗号：('new_label',),否则会报错：　　

```
ValueError: Unknown initialization pattern for softmax_label. Default initialization is now limited to "weight", "bias", "gamma" (1.0), and "beta" (0.0).
Please use mx.sym.Variable(init=mx.init.*) to set initialization pattern
```

## 关于自定义loss通过mx.sym.MakeLoss  

先列出检索到的相关issue:  

1.[issue4737](https://github.com/dmlc/mxnet/issues/4737)  
2.[stackoverflow](http://stackoverflow.com/questions/42304820/how-to-weight-observations-in-mxnet/42323339#42323339)

```python
# -*- coding=utf-8 -*-

import mxnet as mx
import numpy as np
import logging

logging.basicConfig(level=logging.INFO)

x = mx.sym.Variable('data')
y = mx.sym.FullyConnected(data=x, num_hidden=1)
label = mx.sym.Variable('label')
loss = mx.sym.MakeLoss(mx.sym.square(y - label))
pred_loss = mx.sym.Group([mx.sym.BlockGrad(y), loss])
ex = pred_loss.simple_bind(mx.cpu(), data=(32, 2))

# test
test_data = mx.nd.array(np.random.random(size=(32, 2)))
test_label = mx.nd.array(np.random.random(size=(32, 1)))

ex.forward(is_train=True, data=test_data, label=test_label)
ex.backward()

print ex.arg_dict
fc_weights = ex.arg_dict['fullyconnected0_weight'].asnumpy()
fc_weights_grad = ex.grad_arrays[1].asnumpy()
fc_bias = ex.arg_dict['fullyconnected0_bias'].asnumpy()
fc_bias_grad = ex.grad_arrays[2].asnumpy()

logging.info('fc_weight:{}, fc_weights_grad:{}'.format(fc_weights, fc_weights_grad))
logging.info('fc_bias:{}, fc_bias_grad:{}'.format(fc_bias, fc_bias_grad))
```

以上是自己写的小栗子。需要几点来说明：

mx.sym.MakeLoss仅仅相当于自定义Loss,并没有涉及到prediction,因此在想获取prediction时，需要通过Group将prediction=mx.BlockGrad(y)和
loss合并起来。

* [mx.sym.BlockGrad](http://mxnet.io/api/python/symbol.html#mxnet.symbol.BlockGrad):用于获取symbol的输出，并且回传0梯度。　　
* [mx.sym.Group](http://mxnet.io/api/python/symbol.html#mxnet.symbol.Group):返回symbol列表  

## 制作自己的数据集　　

使用mxnet/tools/im2rec.py,该工具可以用来生成xx_.lst和xx.rec件,所以有两套参数设置来获得lst、rec.  
对于lst文件，是用im2rec.py中的`make_list()`方法获得的,其中涉及三个参数：  

* args.root: 指定数据集的根目录，其子目录为图片或进一步的子目录　　
* args.recursive: 是否递归访问子目录，如果存在多个目录可以设置该参数　　
* args.exts: 数据集图片格式[jpg, jepg]  

当然在制作lst时，需要设置list=True.运行该命令即可得到对应lst:  

```
python --recursive=True --list=True prefix root
```
替换prefix为要生成的lst文件的名字（可以包含路径），root对应图片的根目录。　　

对于rec文件，则需要设置剩余参数，但需要将list=False,此时将会使用已经生成的lst文件。反之，则创建lst然后生成rec.  

```
usage: im2rec.py [-h] [--list LIST] [--exts EXTS] [--chunks CHUNKS]
                 [--train-ratio TRAIN_RATIO] [--test-ratio TEST_RATIO]
                 [--recursive RECURSIVE] [--shuffle SHUFFLE]
                 [--pass-through PASS_THROUGH] [--resize RESIZE]
                 [--center-crop CENTER_CROP] [--quality QUALITY]
                 [--num-thread NUM_THREAD] [--color {-1,0,1}]
                 [--encoding {.jpg,.png}] [--pack-label PACK_LABEL]
                 prefix root
```
以上是所有参数，常用设置--train-ratio为train\val数据集的划分比例，--quality表示图片质量，--center
-crop表示是否对图片进行中心切分。剩下还是看代码中参数吧，仅做笔记。  

## 关于Embedding  

由于Embedding是将word映射为向量，此时输入为离散变量，因此该层对于输入是不可求导的。那么在使用时应该注意如下：　　

```python
mx.sym.Embedding(data=mx.sym.BlockGrad(sentence), input_dim, out_dim, name='embedding)
```
这里可以看到，输入使用`mx.sym.BlockGrad`进行包裹，目的在于获取sentence的输出，但是可以同时保证不对其进行求导。不仅如此，对于一些不可导的输入我们都可以利用其进行包裹。  

## 关于SoftmaxOutput  

最近才发现SoftmaxOutput使用有些小问题，其输入标签如果是one-hot形式，则会导致各种backward问题，说白了，
就是因为one-hot形式与SoftmaxOutput输出形式不同，如7类，one-hot形式为[0, 0, 0, 1, 0, 0, 0], 而对应SoftmaxOutput输出为
logit, [0.2, 0.3, 0.1, 0.2, 0.2, 0.1, 0.1], 这样在计算loss时会取最大位置4,其余位置为0, 而与one-hot中的0进行计算，计算准确率会发现$$acc \approx 0.83333$$，原因在于将0的位置计算为了正确的标签。即将one-hot的标签形式当做不是one-hot形式的计算，
当做多标签任务计算。所以，在提供标签时，应该直接提供标签数字形式，而非one-hot。(这是本年度最匪夷所思的错误)


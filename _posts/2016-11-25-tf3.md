---
title: " tensorflow使用笔记(3)--Reading data"
layout: post
date: 2016-11-25
image: /assets/images/markdown.jpg
headerImage: false
tag: tensorflow
blog: true
author: karl
description: 读取数据，从文件，从队列
---  

昨天试着用python生成器读取10k张图片，结果raise out of memory, 我在想生成器应该没这么惨吧。虽然使用的是keras，那也不至于直接out of memory, 但是后来使用caffe反倒没有报错，这是和caffe的训练机制有关系：caffe的一次迭代不是正常框架的一次迭代，而是一个batch,对应solver文件里的`test_iter`表示的也是多少个test batches,我就说那么多图片不可能几秒钟就读完。进入正题，今天来看看tensorflow如何保存数据到官方数据格式，然后进行读取。(ps:虽然是翻译的文档，但也算是一种理解)    

----

## Summary  

- Reading data
    - Feeding
    - Reading from files
        - Filenames, shuffling, and epoch limits
        - File formats
        - Proeprocess
        - Batching
        - Creating threads to prefetch using QueueRunner objects
        - Sparse input data
    - Preloaded data
    - Multiple input piplines
----  

### Feeding  

tensoflow中的feed机制主要用于为`placeholder`变量提供数据，并在session运行是为其提供数据，声明方式以字典形式，如theano中的given类似：  

```python
with tf.Session() as sess:
    sess.run(trainer, feed_dict={placeholder_a: data_a, placeholder_b: data_b})
```  
因为tensorflow是先构造graph，然后运行，因此需要这种机制来为图提供数据。  

### 从文件中读取数据  
从文件中读取数据的流程基本可以如下面步骤：  

1. 得到文件名列表  
2. 对列表进行shuffle  
3. 设置epoch  
4. 构造队列用于存放文件名  
5. 构造一个读取文件的reader  
6. 从队列中读取文件名  
7. 对读取到的数据进行预处理    

#### 文件名， shuffle， epoch  

对于文件名列表，通常表示方法可以像这样：`['file0, 'file1]`或者`[('file%d' % i ) for i in xrang(2)]`,在tensorflow中可以使用`tensorflow.train.match_filenames_once`,其官方说明如下：    

```python
tf.train.match_filenames_once(pattern, name=None)
```  
功能：存取与pattern匹配的文件列表，只计算一次    
- Args:  
    - `pattern`: 一个文件名模式，可以使用glob获得  
    - `name`: 名字  

- Returns:  
    - 返回一个变量用于匹配pattern  

在获取匹配模式的变量之后，我们可以得到对应模式的文件名，下一步便是将filenames进行读取，tensorflow中，有`tf.train.string_input_producer`,可以构建一个FIFO队列用作存取文件名，我们可以看一下文档说明：  
```python
tf.train.string_input_producer(
    string_tensor,
    num_epochs=None,
    shuffle=True, seed=None,
    capacity=32,
    shared_name=None,
    name=None)
```  
功能：输出文件名到一个队列中。  

- Args:
    - `string_tensor`: 一个1-D可以生成string的string tensor。
    - `num_epoches`: 如果指定的话，每个字符串会重复生成num_epoches次数；反之，每个字符会被无限制的生成。
    - `shuffle`: 是否在每个epoch shuff strings。  
    - `seed`: 随机种子，只有`shuffle=True`才有用  
    - `capacity`: 设置队列的容量  
    - `shared_name`: 如果设置，该队列可以在不同的session下被共享，这里的名字应该是session的名字。  
    - `name`: 队列名字  
- Returns:  
    - 返回一个队列，同时一个绑定该队列的QueueRunner会被加入到graph 的QueueRunner中。
- Raises:  
    - ValueError: 如果strings_tensor是一个空列表，运行时将会raise ValueError。  

#### 文件格式  

选择合适的reader读取对应的文件格式，并且将获取的文件名队列传给reader的read方法。通常，read方法会返回一个文件标识符，一个record用作debug，还有一个字符串变量；通过一个decoder和一个conversion ops将字符串变量decode至tensors，即实现了文件名与图的接轨。  

##### CSV files  

对于csv格式的文件，可以使用`TextLineReader`，`decode_csv`可以读取。这两个方法如下说明：  

```python
class tf.TextLineReader.read(queue, name=None)
```  
tf.TextLineReader是一个类，继承至`tf.ReaderBase`,这里只介绍read方法，其余方法可以参考[API](https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops.html#TextLineReader)  

- Args:
    - `queue`: 一个字符串队列或者一个可变字符串tensor，用来表示一个队列的句柄。  
    - `name`: 操作的名字。  
- Reutrns:  
    - 返回tensor(key, values)    

```python
tf.decode_csv(
    records,
    record_defaults,
    field_delim=None,
    name=None
    )
```  
将csv转换成tensors,每一列对应一个tensor  

- Args:  
    - `records`: string类型的tensor,每一个string是具有csv类似格式的。  
    - `record_defaults`: tensor的类型  
    - `field_delim`: 默认分隔符，csv为`,`。  
    - `name`: 操作符名称  
- Retures:  
    - 返回tensor列表，每一个tensor类型和`record_defaults`相同，每一个tensor与 `records`具有相同的大小。  

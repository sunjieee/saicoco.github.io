---
layout: page
title: SoundNet--Learning Sound Representations from Unlabeled Video
tag: paper
categories: 
- paper_reading
comments: true
blog: true
data: 2016-10-30
---  

十月就过去了，好快～回顾这一个月，除了增强了python,论文方面被老板带来带去没什么进展，不由得想去知乎提问"研究生遇到一个很水的导师是什么体验"……
开个玩笑^_^.最近ECCV上挺多关于sound->video的文章，上篇文章是video->sound,不由的觉得自己发现新大陆一般，赶巧我在这块正发愁呢，啥话不说，开讲啦!

## SoundNet　　

文章[^1]的目的在于学习video与sound之间学习acousic representation,为的是声音与video可以对应起来，可以用作声音检索视频等一系列应用。同时，文章提出了一种
student-teacher training procedure,这个训练方法后面会讲，主要用来将别的训练好的模型迁移到别的模型，使得别的模型具有原来训练好的模型的性能或者能力。这篇
blog分为一下几部分:  

* SoundNet框架　　

* student-teacher训练方法　　

### SoundNet框架　　

结合图最好解释了，先上图1:  

![1](/downloads/soundnet/1)  

由上图我们可以看出soundnet的组成，输入为没有标签的video,　包含视频和音频，对于每一帧视频，通过已经训练好的CNN(ImageNet CNN, Places CNN),进行目标分类和
场景分类，利用得到的最后一层的特征或者是logits用作sound部分的映射目标；对于音频部分，文章没有利用常用的语音特征(MFCCs, Spectrogram .etc),而是直接使用
原始wavform,在输入到1D CNN后通过一系列conv和pooling,最后得到两个输出，用作与video部分的映射进行metrics learning，这里度量函数为KL loss.当然，文章这么做，
是因为有足够的音频和视频数据，换做以往小量音视频数据，这么做时能基于pre-train或者语音特征使用MFCC等手工特征。而模型结构具体参数可以如下述代码所示，而代码工程在[soundnet](https://github.com/cvondrick/soundnet)  



```lua

    net = nn.Sequential()

    net:add(nn.SpatialConvolution(1, 32, 1,64, 1,2, 0,32))
    net:add(nn.SpatialBatchNormalization(32))
    net:add(nn.ReLU(true))
    net:add(nn.SpatialMaxPooling(1,8, 1,8))

    net:add(nn.SpatialConvolution(32, 64, 1,32, 1,2, 0,16))
    net:add(nn.SpatialBatchNormalization(64))
    net:add(nn.ReLU(true))
    net:add(nn.SpatialMaxPooling(1,8, 1,8))

    net:add(nn.SpatialConvolution(64, 128,  1,16, 1,2, 0,8))
    net:add(nn.SpatialBatchNormalization(128))
    net:add(nn.ReLU(true))
    net:add(nn.SpatialMaxPooling(1,8, 1,8))
    net:add(nn.SpatialDropout(0.5))

    net:add(nn.SpatialConvolution(128, 256,  1,8, 1,2, 0,4))
    net:add(nn.SpatialBatchNormalization(256))
    net:add(nn.ReLU(true))
    net:add(nn.SpatialDropout(0.5))

    net:add(nn.ConcatTable():add(nn.SpatialConvolution(256, 1000, 1,16, 1,12, 0,4))
                            :add(nn.SpatialConvolution(256,  401, 1,16, 1,12, 0,4)))

    net:add(nn.ParallelTable():add(nn.SplitTable(3)):add(nn.SplitTable(3)))
    net:add(nn.FlattenTable())

```　　

由上述代码可以看出，sound部分没有全连接层，原因在于每个audio的长度不定，因此文章使用全卷积网络，这里困惑的是不定长的audio对应video
必然也是不定长的，文章是不是恰巧是对应的，这里后续在[issue](https://github.com/saicoco/saicoco.github.io/issues/7)中作补充。其实这里看出，
文章其实包装比较好，核心其实是一堆audio部分的CNN,还有对应的"标签"来自video CNN部分，然后利用student-teacher进行训练。　　

### Student-Teacher  

这个训练方法属于transfer learning部分，手法在于利用已有模型得到的预测结果作为student模型的弱标签，使得student去拟合teacher的复杂函数，
以此来达到迁移的目的。可以参看[Do Deep Nets Really Need to be Deep?](http://xueshu.baidu.com/s?wd=paperuri%3A%28078415e6ab570770529798299e0d8b90%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Farxiv.org%2Fabs%2F1312.6184&ie=utf-8&sc_us=7007594391503052629)，
文章[^3]主要描述了一种训练方法，使得简单浅层网络可以达到深层复杂模型的性能，利用deep Model最后的输出logits作为原来输入数据的新标签，去重新输入到
shallow model中，以此使得浅层模型可以得到由原始数据到logits的映射函数，即得到了deep model的映射能力，正是"如果我不能打败你，我就加入你"，当然，还有这篇
文章[^2]，都是在学习一种模型去拟合复杂模型达到迁移学习的目的。　　

回到文章[^1]中，文章将这种技巧用到sound->video,利用已经训练好的场景分类和目标分类的CNN模型得到video对应的logits,然后将对应的audio
输入至audio CNN中，在最后的输出部分去回归logits,从audio的角度去拟合映射函数，达到sound到场景sematic level label，可以说比较创新。　　

### 写在后面的话　　

最近梦比较乱，不像以前的很整齐，要么各个阶段的好朋友，要么家人亲戚，想家了！　　


## Reference  

[^1]: SoundNet: Learning Sound Representations from Unlabeled Video" by Yusuf Aytar, Carl Vondrick, Antonio Torralba. NIPS 2016  

[^2]: Hinton G, Vinyals O, Dean J. Distilling the Knowledge in a Neural Network[J]. Computer Science, 2015, 14(7):38-39.  

[^3]: Ba L J, Caruana R. Do Deep Nets Really Need to be Deep?[J]. Advances in Neural Information Processing Systems, 2014:2654-2662.  　　

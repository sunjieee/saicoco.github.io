---
title: "Pytorch使用笔记"
layout: post
date: 2017-03-22
tag: Algorithm
blog: true
star: false
author: karl
category: Pytorch
description: pytorch
---  

最近开始使用pytorch，原因在于发现它在gpu上跑起来快的飞起啊，于是想着开个文章记录使用过程中的细节问题，
鉴于网上有很多helloworld教程，本文就不对这块做记录了。　　

　
先上个复杂的网络瞅瞅：　　
```python

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

class Net(nn.Module):
    def __init__(self, batch_size, n_classes, feature=75, seq_len=49, num_hidden=[256, 256]):
        super(Net, self).__init__()
        
        self.batch_size = batch_size
        self.num_hidden = num_hidden
        # LSTM
        self.lstm1 = nn.LSTM(input_size=75, hidden_size=num_hidden[0], num_layers=seq_len)
        self.lstm2 = nn.LSTM(input_size=num_hidden[0], hidden_size=num_hidden[1], num_layers=seq_len)

        h1 = torch.randn(seq_len, batch_size, num_hidden[0])
        h2 = torch.randn(seq_len, batch_size, num_hidden[0])
        c1 = torch.randn(seq_len, batch_size, num_hidden[0])
        c2 = torch.randn(seq_len, batch_size, num_hidden[0])
        if torch.cuda.is_available():
            h1 = h1.cuda()
            h2 = h2.cuda()
            c1 = c1.cuda()
            c2 = c2.cuda()
        self.h1 = Variable(h1)
        self.h2 = Variable(h2)
        self.c1 = Variable(c1)
        self.c2 = Variable(c2)

        # attention conv
        self.conv1 = nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1)
        self.conv1_drop = nn.Dropout2d()
        self.conv2 = nn.Conv2d(512, 2, 1)
        self.conv2_drop = nn.Dropout2d()

        self.fc1 = nn.Linear(in_features=1024, out_features=256)
        self.fc1_drop = nn.Dropout()
        self.fc2 = nn.Linear(in_features=512, out_features=256)
        self.fc2_drop = nn.Dropout()
        self.fc3 = nn.Linear(in_features=256, out_features=400)
        self.fc3_drop = nn.Dropout()
        self.fc4 = nn.Linear(400, n_classes)

    def forward(self, x, y):
        """
        x: audio data [seq_len, batch_size, feature_length]
        y: face data [batch_size, c, h, w]
        """
        # encoding audio data
        lstm1_out, lstm1_c = self.lstm1(x, (self.h1, self.c1))
        lstm2_out, lstm2_c = self.lstm2(lstm1_out, (self.h2, self.c2))
        lstm1_output = torch.squeeze(torch.mean(lstm1_out, dim=0))
        lstm2_output = torch.squeeze(torch.mean(lstm2_out, dim=0))
        lstm_output = torch.cat([lstm1_output, lstm2_output], 1)

        # tile
        repeate_lstm = lstm_output.view(self.batch_size, 512, 1, 1)
        repeate_lstm.data = repeate_lstm.data.repeat(1, 1, 14, 14)

        # concatenate with face data
        concat_out = torch.cat([y, repeate_lstm], 1)
        conv1 = F.relu(self.conv1_drop(self.conv1(concat_out)))
        conv2 = F.relu(self.conv2_drop(self.conv2(conv1)))
        conv2_reshape = conv2.view(-1, 2, 196)
        attention_vec = F.softmax(conv2_reshape)
        face_reshape = y.view(-1, 512, 196)
        attention  = torch.randn(self.batch_size, 512, 2)
        if torch.cuda.is_available():
            attention = attention.cuda()
        attention = torch.baddbmm(attention, face_reshape.data, torch.transpose(attention_vec, 2, 1).data)
        attention = attention.view(self.batch_size, -1)

        attention = Variable(attention)
        # low rank
        image_embed = F.relu(self.fc1_drop(self.fc1(attention)))
        audio_embed = F.relu(self.fc2_drop(self.fc2(lstm_output)))
        hadamand_prod = F.relu(image_embed * audio_embed)
        fusion_feature = F.relu(self.fc3_drop(self.fc3(hadamand_prod)))
        softmax_out = F.log_softmax(self.fc4(fusion_feature))
        return softmax_out
```

这段代码是我实验根据MCB改的，同时其中有HADAMARD PRODUCT FOR LOW-RANK BILINEAR POOLING的影子，网络接收
两个数据源，这里是人脸数据与语音数据，对于语音部分进行lstm编码解码，对于得到的结果进行tile,然后与人脸
高层特征进行拼接、卷积获取最后的attention向量，利用attention机制获得我们需要关注的人脸部分特征，然后利用
得到的人脸特征与语音数据做hadamand product特征融合，最后得出分类结果。　
　

#### 定义网络结构　　

如果想要定义网络结构，需要继承`torch.nn.Module`,并重写方法`forward()`,必要时写`backward()`方法。在类
定义阶段对即将使用的方法进行定义声明，这里不得不提的有两个东西：`torch.nn`和`torch.nn.functional`,后者
相当于前者声明之后在调用，因此对于conv这类参数居多的操作，可以置于`__init__`中进行定义，这样做的好处在于
代码简洁，同样的，RNN等需要声明hidden_weight和center_vector,所以可以也在类定义阶段实现功能。总的来说，灵活，
如你所愿。　　

#### cpu与gpu无缝切换　　

对于cpu来说，pytorch使用的是`torch.Tensor`,而GPU则是使用`torch.cuda.Tensor`,为了实现无缝切换，即在有gpu的时候
我们使用gpu,没有则使用cpu,这里我使用的是`torch.cuda.is_avaliable()`，如果gpu可用则会返回True,这种情况我们可以使用`torch.Tensor.cuda()`方法将cpu Tensor转换为`cuda.Tensor()`,然后统一`Variable()`.  

#### 读自己的数据集　　

如最开始做mnist classification,例子中使用数据为文件夹，我想尝试下自己的数据，只需要按往常一样读数据。在输入
网络时将其转换为Tensor即可，这个过程会遇到类型不一致的报错，提示信息中很充分，常为对应位置的类型，随机应变就好啦。如label数据类型需为LongTensor,反之会报错的。GPU上Tensor需要切换为cuda.Tensor,反之会包KeyError.  

...未完待续
